{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Model\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "\n",
    "fileName = \"Trump.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# Global MobileNets model\n",
    "mobile = keras.applications.mobilenet.MobileNet(weights=\"imagenet\")\n",
    "model = Model(mobile.input, mobile.layers[27].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_input(fileName, X):\n",
    "    fhandler = open(fileName, \"w\")\n",
    "    for i in range(3):\n",
    "        for j in range(224):\n",
    "            for k in range(224):\n",
    "                fhandler.write(str(X[j][k][i]) + \"\\n\")\n",
    "    fhandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Input_Files():\n",
    "    \n",
    "    img_width = 224\n",
    "    img_height = 224\n",
    "\n",
    "    img = image.load_img(fileName, target_size=(img_width, img_height))\n",
    "    X = image.img_to_array(img)\n",
    "\n",
    "    # Normalizing the Input\n",
    "    X = tf.keras.utils.normalize(X, axis=2)\n",
    "    \n",
    "    # Write Normalized Input to file\n",
    "    write_to_file_input(\"data/FirstLayer/inputNorm.txt\", X)\n",
    "    \n",
    "    \n",
    "    # Write combination input to file\n",
    "    X[:, :, 0] = 1\n",
    "    X[:, :, 1] = 10\n",
    "    X[:, :, 2] = 100\n",
    "    write_to_file_input(\"data/FirstLayer/inputComb.txt\", X)\n",
    "    \n",
    "    # Write all 1's input to file\n",
    "    X[:, :, 0] = 1\n",
    "    X[:, :, 1] = 1\n",
    "    X[:, :, 2] = 1\n",
    "    write_to_file_input(\"data/FirstLayer/inputSet1.txt\", X)\n",
    "\n",
    "    print(\"Creating Input files Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_Input_Files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_one(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(32):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = model.layers[2].get_weights()[0]\n",
    "write_to_file_weights_layer_one(\"data/FirstLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_one(\"data/FirstLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_second(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(32):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = model.layers[5].get_weights()[0]\n",
    "write_to_file_weights_layer_second(\"data/SecondLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_second(\"data/SecondLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkCalculations():\n",
    "\n",
    "    img_width = 224\n",
    "    img_height = 224\n",
    "\n",
    "    img = image.load_img(fileName, target_size=(img_width, img_height))\n",
    "    X = image.img_to_array(img)\n",
    "\n",
    "    # Normalized the Input\n",
    "    X = tf.keras.utils.normalize(X, axis=2)\n",
    "    \n",
    "    # Set as 1\n",
    "    #X[:,:,0] = 1\n",
    "    #X[:,:,1] = 1\n",
    "    #X[:,:,2] = 1\n",
    "\n",
    "    # Set as Combination\n",
    "    #X[:,:,0] = 1\n",
    "    #X[:,:,1] = 10\n",
    "    #X[:,:,2] = 100\n",
    "\n",
    "    print(X[0][0][0])\n",
    "    print(X[0][1][0])\n",
    "    print(X[0][2][0])\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    print(X.shape)\n",
    "\n",
    "    # Weights part\n",
    "    W = model.layers[2].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[2].set_weights(W)\n",
    "    \n",
    "    \n",
    "    W = model.layers[5].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[5].set_weights(W)\n",
    "    \n",
    "    W = model.layers[8].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[8].set_weights(W)\n",
    "    \n",
    "    W = model.layers[12].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[12].set_weights(W)\n",
    "    \n",
    "    W = model.layers[15].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[15].set_weights(W)\n",
    "    \n",
    "    W = model.layers[18].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[18].set_weights(W)\n",
    "    \n",
    "    W = model.layers[21].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[21].set_weights(W)\n",
    "    \n",
    "    W = model.layers[21].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    #W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[21].set_weights(W)\n",
    "    \n",
    "    \n",
    "    output = model.predict(X)\n",
    "    print(\"Output Shape ---> \")\n",
    "    print(output.shape)\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    # Save Output to file\n",
    "    fOutput = open(\"data/SeventhLayer/Seventh_Layer_Output.txt\", \"w\")\n",
    "    for i in range(len(output)):\n",
    "        for j in range(len(output[0][0][0])):\n",
    "            for k in range(len(output[0])):\n",
    "                for l in range(len(output[0][0])):\n",
    "                    fOutput.write(str(output[i][k][l][j]) + \"\\n\")\n",
    "    fOutput.close()\n",
    "    print(\"Output File Writing Complete!!!\")\n",
    "\n",
    "    print(\"\\n\\n Description of Layers\")\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py:2198: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.145017\n",
      "0.351735\n",
      "0.563547\n",
      "(1, 224, 224, 3)\n",
      "Output Shape ---> \n",
      "(1, 56, 56, 128)\n",
      "--------------------------------\n",
      "Output File Writing Complete!!!\n",
      "\n",
      "\n",
      " Description of Layers\n",
      "<keras.engine.input_layer.InputLayer object at 0x7fe70db7d320>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe70dbd3da0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe70dbd3be0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70db8ba20>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70dcf3d68>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe80c872518>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70db0fcf8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70da4be10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe70da6aac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70da2f358>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70d9f27f0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe70d96db38>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe70d93fbe0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70d89fc88>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70d89fb38>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe70d841e48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70d805240>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70d7c8710>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe70d744a20>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70d764780>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70d6fdcf8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe70d648e80>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe70d660b70>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe70d626320>\n",
      "Total Time --> 10.095700740814209\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "checkCalculations()\n",
    "end = time.time()\n",
    "print(\"Total Time -->\", (end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 6 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SixthLayer():\n",
    "    fExp = open('data/SixthLayer/Sixth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SixthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    mismatchStart = 0;\n",
    "    boo = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            if(boo < 500):\n",
    "                boo += 1\n",
    "                print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) + \" <---- \" + str(i) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    print(mismatchStart)\n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500014  --->   0.499986 <---- 5299\n",
      "2.500029  --->   2.5 <---- 12677\n",
      "0.499984  --->   0.500016 <---- 30045\n",
      "0.499934  --->   0.500131 <---- 248054\n",
      "0.499856  --->   0.500051 <---- 249042\n",
      "0.500026  --->   0.499958 <---- 341187\n",
      "Number of mismatch - 6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "checkFile_SixthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 5 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FifthLayer():\n",
    "    fExp = open('data/FifthLayer/Fifth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FifthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkFile_FifthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 1 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FirstLayer():\n",
    "    fExp = open('data/FirstLayer/First_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FirstLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatch - 388864\n"
     ]
    }
   ],
   "source": [
    "checkFile_FirstLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 4 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FourthLayer():\n",
    "    fExp = open('data/FourthLayer/Fourth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FourthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkFile_FourthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkFile_SecondLayer():\n",
    "    fExp = open('data/SecondLayer/Second_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SecondLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFile_SecondLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_ThirdLayer():\n",
    "    fExp = open('data/ThirdLayer/Third_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/ThirdLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 7 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SeventhLayer():\n",
    "    fExp = open('data/SeventhLayer/Seventh_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SeventhLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.500190--->3.4999\n",
      "1.500236--->1.4999\n",
      "0.499970--->0.500031\n",
      "0.499940--->0.500207\n",
      "0.499817--->0.500115\n",
      "0.499837--->0.50012\n",
      "0.500035--->0.499933\n",
      "0.500011--->0.49995\n",
      "0.500019--->0.499936\n",
      "0.500094--->0.499951\n",
      "0.500072--->0.499968\n",
      "1.499694--->1.50002\n",
      "1.499834--->1.50016\n",
      "0.499997--->0.500042\n",
      "4.500089--->4.5\n",
      "3.500033--->3.49998\n",
      "2.499985--->2.50001\n",
      "1.500219--->1.49998\n",
      "3.499984--->3.50002\n",
      "Number of mismatch - 19\n"
     ]
    }
   ],
   "source": [
    "checkFile_SeventhLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 6 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.17376578  0.92570347  1.33578944  0.76546639  0.88438272  1.28590727\n",
      "  0.64942753  1.40510738  1.05130637  1.72453225  0.9786219   1.67618835\n",
      "  0.58891213  1.50489914  1.79923272  2.04591084  1.34045398  0.81471074\n",
      "  0.99819231  0.89749551  1.47205436  1.69671047  1.45804167  0.6779514\n",
      "  0.94730163  1.63231707  2.26928258  1.84552538  1.11021316  1.17338729\n",
      "  1.29545557  2.00494432  2.32145643  1.48001683  1.37820995  1.05569613\n",
      "  1.46535122  0.66719675  0.95240086  7.10886765  1.99117792  0.85256547\n",
      "  2.21908903  1.20690072  1.69672048  0.94427353  3.62987256  0.92673427\n",
      "  1.22302294  1.07908344  1.08553994  1.00814819  1.09811568  1.27662528\n",
      "  1.04946864  1.33240819  1.17767119  1.200719    1.43957746  0.66142839\n",
      "  1.62856257  1.56319773  1.19957316  1.26731813  1.12130773  1.65475392\n",
      "  1.28627729  2.31529236  4.39884281  1.97971916  1.06841552  0.95823908\n",
      "  1.17420721  1.62769365  1.38963318  1.01462626  0.86462164  1.54941881\n",
      "  1.35129344  1.17098308  0.76067418  0.94015956  0.72020191  1.56491935\n",
      "  2.03240156  1.3279444   1.13324714  1.38579226  1.5140686   1.45908129\n",
      "  1.27209127  0.83640563  1.67506766  1.13486207  1.04992676  1.06743562\n",
      "  0.72675216  1.77436161  0.75012028  0.86915714  1.18154442  0.50819468\n",
      "  1.73744273  1.22357368  0.76849401  0.89601618  1.07886708  1.37949109\n",
      "  1.12686002  1.53349817  0.70203727  1.60559881  0.77601296  1.62249327\n",
      "  3.12489629  1.22871459  1.65109372  1.04824734  0.58185148  0.95162141\n",
      "  0.69721955  3.97329402  0.85076779  1.47501397  0.69098943  1.29685533\n",
      "  1.29523468  2.28140354]\n",
      "[ 0.72143465  0.01363674  0.05389157  5.05492496  2.49030423 -1.059587\n",
      " -0.40610358 -0.26670736 -0.23595124 -0.43631679  3.56390285  0.15713345\n",
      "  0.04360056 -0.015995   -0.07100134  2.56553316 -0.35836366 -1.17873263\n",
      "  0.15068009  2.44716811 -0.05382602  0.00852313 -0.15267693  2.2692802\n",
      "  0.98935497  0.45503432  1.77241206  0.48598987 -0.19603463  0.62342739\n",
      " -0.28581443 -1.68366933 -1.29396343  5.51000786 -0.02630313  1.71006966\n",
      " -0.45583376  1.74399924  1.91439617  2.29675508  0.87310314  1.45533311\n",
      "  2.4629128  -0.00800036  1.76306212 -1.18711591  3.93522739  3.79581332\n",
      "  0.02762886  1.53619325  0.31614617 -0.08858164  0.86725718 -0.08668464\n",
      "  3.14633536  5.34504795 -0.00735868  0.58026201  0.88394499  1.524876\n",
      " -0.2799972   0.63548577 -0.8487798  -0.0197631  -0.03428796 -0.17134026\n",
      " -0.01232282 -0.20907333  1.322492    0.19782865  0.1726096   0.46545157\n",
      "  2.98773122  0.05923635  0.0192916   2.11264873  3.45107317  0.15503553\n",
      "  1.16716337  0.79630387  3.41199231  2.28103614 -1.18197358  0.64179146\n",
      "  0.0349035   0.06411094 -0.01636218  0.01400144  0.0223705   0.75868112\n",
      "  0.03154705 -1.12465    -0.33665654 -0.03624693  3.19853497  0.04825925\n",
      "  2.91152072  0.299346    3.52241039  0.61567116  4.98891258  3.29594588\n",
      " -0.1974256   5.14031601  3.26431298  0.46737498  1.58523679 -0.09522966\n",
      "  0.72984499 -0.08363263  5.13377666  2.6981988   2.05323291  0.96600771\n",
      "  2.32890534 -1.59002006 -0.05867134  0.25444266  2.71917868  0.3952128\n",
      "  1.01329684  1.47243226  3.60865808 -0.04209217  4.90215921 -0.09545956\n",
      "  0.03944332 -0.2469881 ]\n",
      "[ -1.62857456e+01   2.36994550e-01   3.68160933e-01  -1.58168221e+00\n",
      "  -3.64298415e+00   1.02406514e+00   3.36134225e-01   7.03423977e+00\n",
      "   1.72216690e+00  -1.53163648e+00  -1.91409457e+00   1.92484006e-01\n",
      "   4.56330739e-02   2.82312840e-01  -1.82308555e+00  -2.26906109e+00\n",
      "   3.29711318e+00   9.53502581e-02   5.39282036e+00   1.59590340e+01\n",
      "  -5.78078270e-01  -1.00097692e+00   3.91717249e-35  -1.69104505e+00\n",
      "  -1.60801068e-01  -5.24731874e+00  -2.01815918e-01   4.58267093e-01\n",
      "   1.00542545e+00   1.74359590e-01   6.42381048e+00   9.26621914e+00\n",
      "  -2.69595795e+01   1.78625524e+00   1.85959613e+00  -1.44229203e-01\n",
      "   4.61823940e+00   3.10543686e-01  -8.22982043e-02  -1.75826386e-01\n",
      "   7.32099712e-01  -1.72647119e+00   2.04983354e+00  -3.46644968e-02\n",
      "  -3.22400308e+00   3.91766725e-35   1.27896145e-01  -1.62688696e+00\n",
      "   6.38542509e+00  -1.38984251e+01   9.66760874e-01  -3.32768989e+00\n",
      "  -1.53330767e+00   2.70483637e+00   1.99987459e+00  -1.04498279e+00\n",
      "   4.53105688e-01  -2.25654811e-01   4.86887068e-01   5.08301878e+00\n",
      "   1.15261326e+01   1.04510081e+00   2.62209558e+00   3.90204757e-01\n",
      "  -8.99654478e-02  -2.66094893e-01   1.06770444e+00  -6.80564225e-01\n",
      "   3.39944363e-01  -5.42577505e-01   5.43082094e+00   8.67362201e-01\n",
      "  -9.42090005e-02   8.07936311e-01   2.28375182e-01   4.27778065e-02\n",
      "  -7.46880412e-01   9.72988755e-02   1.86985981e+00   3.06159782e+00\n",
      "   8.01661849e-01   1.49351206e+01   3.91756652e-35   1.90995574e-01\n",
      "  -1.07430443e-01   1.00079119e+00  -3.03950691e+00   5.91260970e-01\n",
      "  -2.02048838e-01  -1.33536386e+00   1.82889390e+00   3.91733062e-35\n",
      "  -1.10508022e+01   4.86638211e-03   6.56612921e+00   9.79586184e-01\n",
      "  -4.95578623e+00   1.38369128e-01  -1.84351528e+00   2.26834512e+00\n",
      "  -3.66257572e+00  -1.16146708e+00   1.92451513e+00   3.38294244e+00\n",
      "  -5.02274632e-01   2.50613785e+00   3.72205186e+00   7.17671335e-01\n",
      "   2.50098443e+00  -5.18723869e+00  -1.37048221e+00  -2.91559160e-01\n",
      "   1.92680669e+00  -1.98285460e+00  -3.08727050e+00   1.31152964e+00\n",
      "  -1.42882776e+00   5.41563988e+00  -2.15205574e+00   2.01092219e+00\n",
      "  -5.92545319e+00  -5.48168600e-01   3.58069211e-01  -5.08073688e-01\n",
      "  -9.95631337e-01  -8.23518157e-01   2.30404854e-01  -3.18591565e-01]\n",
      "[  3.76103210e+01   1.16115789e+01   3.29946976e+01   1.05694513e+01\n",
      "   3.80222917e+00   5.01570559e+00   2.82293534e+00   1.15944424e+01\n",
      "   7.49789524e+00   1.40843506e+01   1.24580240e+01   5.31352119e+01\n",
      "   3.56017232e-01   2.69156246e+01   3.57640743e+00   3.29109039e+01\n",
      "   4.33164501e+00   2.71569401e-01   1.94177265e+01   4.46240730e+01\n",
      "   1.82479362e+01   8.35522079e+01   3.91784059e-35   7.65668154e+00\n",
      "   1.29563093e+01   2.17927818e+01   6.23036575e+01   8.90553570e+00\n",
      "   1.44125929e+01   9.73539352e+00   1.67458553e+01   1.35288358e+00\n",
      "   3.32631798e+01   2.65456848e+01   1.47194748e+01   2.57456994e+00\n",
      "   1.09607992e+01   1.23434591e+01   3.61420441e+01   5.57796974e+01\n",
      "   3.87422943e+01   1.03648567e+01   3.68167877e+01   1.24355049e+01\n",
      "   2.82823429e+01   3.91750138e-35   1.08790314e+02   1.37621384e+01\n",
      "   2.17773457e+01   8.00522423e+00   4.09728546e+01   1.04315786e+01\n",
      "   2.72701225e+01   4.09051514e+01   1.19584866e+01   1.87313442e+01\n",
      "   8.32246628e+01   1.32809057e+01   7.25432110e+00   2.53678322e+01\n",
      "   2.61727695e+01   1.40628576e+01   1.13949747e+01   3.17351913e+01\n",
      "   1.19382162e+01   3.10877800e+00   2.61934662e+01   9.24599361e+00\n",
      "   5.83549385e+01   2.73752556e+01   1.71087036e+01   9.68938637e+00\n",
      "   2.64679699e+01   8.77896423e+01   1.43164787e+01   8.36222935e+00\n",
      "   1.64544144e+01   6.07497644e+00   4.14567680e+01   4.25868149e+01\n",
      "   6.91938066e+00   6.45029907e+01   3.91750396e-35   1.22446785e+01\n",
      "   7.09793167e+01   4.72387886e+01   2.06681466e+00   1.81621532e+01\n",
      "   3.37568665e+01   3.52695727e+00   9.28300552e+01   3.91784920e-35\n",
      "   4.41573572e+00   5.15706015e+00   1.06214409e+01   1.22181854e+01\n",
      "   1.37447462e+01   7.64488411e+00   1.07888832e+01   8.24712563e+00\n",
      "   1.00330257e+01   4.06247950e+00   1.05051718e+01   1.90270424e+01\n",
      "   2.46264291e+00   9.16028309e+00   4.98442497e+01   3.18259525e+01\n",
      "   2.99212666e+01   9.13469696e+01   8.12077999e+00   6.87551346e+01\n",
      "   8.64203739e+00   1.25060053e+01   6.45190811e+01   3.18356347e+00\n",
      "   6.86607838e+00   1.19642820e+01   8.85173225e+00   1.68207359e+01\n",
      "   4.25426369e+01   3.87855415e+01   7.90406895e+00   2.54926472e+01\n",
      "   5.51207066e+00   2.25328102e+01   2.42359695e+01   1.46592426e+01]\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[19].get_weights()\n",
    "\n",
    "fMean = open(\"data/SixthLayer/Sixth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SixthLayer/Sixth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SixthLayer/Sixth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SixthLayer/Sixth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[6].get_weights()\n",
    "\n",
    "fMean = open(\"data/SecondLayer/Second_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SecondLayer/Second_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SecondLayer/Second_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SecondLayer/Second_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 3  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_three(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(64):\n",
    "                for l in range(32):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer8 = model.layers[8]\n",
    "W = np.array(layer8.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer8.get_weights()[0]\n",
    "write_to_file_weights_layer_three(\"data/ThirdLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_three(\"data/ThirdLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 3 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[9].get_weights()\n",
    "\n",
    "fMean = open(\"data/ThirdLayer/Third_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/ThirdLayer/Third_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/ThirdLayer/Third_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/ThirdLayer/Third_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 4  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_four(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(64):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer12 = model.layers[12]\n",
    "W = np.array(layer12.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer12.get_weights()[0]\n",
    "write_to_file_weights_layer_four(\"data/FourthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_four(\"data/FourthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 5 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer16 = model.layers[16]\n",
    "W = np.array(layer16.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[16].get_weights()\n",
    "\n",
    "fMean = open(\"data/FifthLayer/Fifth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/FifthLayer/Fifth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/FifthLayer/Fifth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/FifthLayer/Fifth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 4 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer13 = model.layers[13]\n",
    "W = np.array(layer13.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "fMean = open(\"data/FourthLayer/Fourth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/FourthLayer/Fourth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/FourthLayer/Fourth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/FourthLayer/Fourth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 5  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_five(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(128):\n",
    "            for k in range(1):\n",
    "                for l in range(64):\n",
    "                    fTemp.write(str(W[i][k][l][j]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer15 = model.layers[15]\n",
    "W = np.array(layer15.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer15.get_weights()[0]\n",
    "write_to_file_weights_layer_five(\"data/FifthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_five(\"data/FifthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 6  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_six(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(128):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(1):\n",
    "                    fTemp.write(str(W[j][k][i][l]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7fda5f453a90>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fda5f441e10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fda5f441c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fda5f4f3e10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fda5f4cec50>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fda5f527a20>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fda5f592e48>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fda5f36ab00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fda5f317d68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fda5f335b70>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fda5f249e10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fda5f26c780>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fda5f26c8d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fda5f1bdc88>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fda5f1bdfd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fda5f169c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fda5f109a58>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fda5f0a2e10>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fda5f047668>\n",
      "(1, 3, 3, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer18 = model.layers[18]\n",
    "W = np.array(layer18.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer18.get_weights()[0]\n",
    "write_to_file_weights_layer_six(\"data/SixthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_six(\"data/SixthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 7  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_seven(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(128):\n",
    "                for l in range(128):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7fe7f7ffcef0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe78e7ce208>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe78e7ce470>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe78e7ce630>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe78e7cee80>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe78e7ceac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe78e7cef28>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe78df5d6a0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe78c65fa58>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe78c623358>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe78c5e7c88>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe78c5bb588>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe78c5bbfd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe78c4a36a0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe78c4a37b8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe78c4d3fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe78c487be0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe78c44d240>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe78c3aaac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe78c36c438>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe78c300b38>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe78c2aeda0>\n",
      "(1, 1, 1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer21 = model.layers[21]\n",
    "W = np.array(layer21.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer21.get_weights()[0]\n",
    "write_to_file_weights_layer_seven(\"data/SeventhLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_seven(\"data/SeventhLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 8  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_eight(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(128):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(1):\n",
    "                    fTemp.write(str(W[j][k][i][l]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7fe6e3ce8f98>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe6e3cd1550>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e3cd1fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3d2a5c0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3d7d710>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e3d67240>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3b43e80>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3ae7c88>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e3a95ef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3ab1d68>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3a13208>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe6e39e9908>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e39e9978>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e393de10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e393deb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e38e7dd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3887d68>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3820fd0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e37c07f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3783940>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3748278>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e371aef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e36e6fd0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e367fe10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe6e35e3f28>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e35cdc18>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3624f60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3505c50>\n",
      "(1, 3, 3, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer25 = model.layers[25]\n",
    "W = np.array(layer25.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer25.get_weights()[0]\n",
    "write_to_file_weights_layer_eight(\"data/EighthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_eight(\"data/EighthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Norm for Layer 8 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 128)\n",
      "[ 1.337888    0.90943027  0.88350779  0.82119817  1.1963923   1.25710952\n",
      "  0.84129989  1.3480562   0.90536273  2.26521015  1.01386595  0.87453872\n",
      "  0.77734643  0.84597468  0.85774988  0.8275044   1.62166631  1.41489255\n",
      "  0.68778014  0.78766501  1.35182798  0.80219281  1.13184297  0.90223449\n",
      "  1.20009124  0.79529476  0.69522786  0.94525367  0.63746732  0.86005265\n",
      "  1.17265153  0.78047383  0.70633912  0.63958257  0.9263351   0.75938416\n",
      "  0.90326536  0.76355779  0.76144618  0.96458131  0.64441055  0.93267006\n",
      "  0.91538304  1.13677108  0.81600767  1.79164779  1.43400168  1.05403352\n",
      "  0.78488308  1.47575319  0.92726362  1.7832998   0.84020311  1.00519872\n",
      "  1.00548065  0.95009243  0.79216266  1.51746345  0.93091702  0.60446906\n",
      "  1.57686329  1.80588341  0.9934628   0.90483522  0.86138862  0.85959095\n",
      "  0.82631963  1.00321925  0.72036535  0.96975553  1.0304358   0.74167258\n",
      "  0.79325265  0.70482165  0.80507636  0.98704749  0.79373795  1.65253055\n",
      "  0.88163197  2.0150454   1.058213    1.19508696  1.63018489  0.76262504\n",
      "  1.06820869  0.905931    0.72671998  1.10162735  0.90507537  0.84585828\n",
      "  1.62141824  1.12503576  0.9370569   0.83803928  0.86509889  1.79976809\n",
      "  0.90316015  0.75853884  0.84041911  1.21525955  1.44894445  1.01123881\n",
      "  0.75375199  0.97619194  0.83100671  0.77575952  1.5430218   1.89660132\n",
      "  0.72172582  0.92678803  1.0580256   0.7773279   1.9673928   0.82498461\n",
      "  0.82598817  1.0674963   0.77033418  0.71995264  1.58693182  0.84420407\n",
      "  0.87477952  1.34508801  2.01002431  0.79405934  0.82889879  0.82628483\n",
      "  0.85110688  1.26921833]\n",
      "[ 0.58317757  1.9215827   3.43787193  0.87749457  0.61694402  0.26172683\n",
      "  2.24897456 -0.24478418  1.13950825 -0.38827509  0.88666558  0.94437563\n",
      "  1.34902871  1.2039367   0.90261453  0.9412865   1.0556736  -0.29676279\n",
      "  0.58750629  0.79776853 -0.31100768  2.07961822  1.38837659  1.09417832\n",
      " -0.39391699  5.09257269  0.36316988  2.4282856   1.25087392  0.46531421\n",
      " -0.37519476  0.62665015  3.91053891  1.52885306  0.97249645  1.45728636\n",
      "  1.31733799  0.64876544  1.97274172  1.56709433  0.44982064  1.29919016\n",
      "  3.79453993  1.33657491  0.81058723  0.14567086 -0.40190974  1.7005055\n",
      "  2.99678111  0.73956722 -0.04094648 -0.15897132  1.26910818  3.16368389\n",
      "  1.42108107  0.96424347  0.31174612 -0.48191208  4.15666533  0.18932866\n",
      "  0.48391837 -0.26190639  0.13581154  1.19568229  3.39605737  3.09718609\n",
      "  3.4108088   3.06777668  0.80621624  2.01991415  0.78716344  2.58304691\n",
      "  5.07393837  2.90362072  0.29152966  0.79166204  3.1477356  -0.59454554\n",
      "  0.80085826 -0.94832283  0.14052269  0.53365874 -0.12843694  0.98098224\n",
      "  4.85600996  0.87878031  0.6319108   0.94029003  1.95415771  0.69735056\n",
      " -0.48354483 -0.20881878  2.09417081  0.76866221  0.70303106 -0.04524006\n",
      "  0.97172219  4.66405869  0.81066835 -0.40926296  0.74231458  2.03548193\n",
      "  3.69042587  2.85758781  3.18593812  1.0240103  -0.14610384 -0.39406329\n",
      "  1.40849435  0.63980424  0.50270814  0.62651628 -0.45166695  0.96375799\n",
      "  0.82134354  1.10250854  0.58332151  2.88516378  0.09849777  2.71086097\n",
      "  3.95369887  0.21228525 -0.70178354  0.61824787  1.19206321  0.66951001\n",
      "  1.01559591 -1.84938836]\n",
      "[ -1.14055395e+01   2.14793129e+01  -4.21152782e+00   8.43809223e+00\n",
      "  -1.45398598e+01  -3.32411537e+01   9.25103188e+00  -1.83876181e+00\n",
      "   4.45322561e+00  -9.61665630e+00  -7.22770786e+00   3.77475142e+00\n",
      "   4.94670296e+00   6.57970190e+00   3.60417223e+00   3.43886876e+00\n",
      "  -5.23686314e+00  -3.60217590e+01   1.12957406e+00   2.71761417e+00\n",
      "  -2.27819538e+00   3.52266240e+00   8.30148411e+00   4.33282566e+00\n",
      "   5.33395576e+00  -3.19844341e+00   3.91816575e-35  -1.70123825e+01\n",
      "   4.06720877e+00   1.78373349e+00  -4.63693470e-01   2.50775099e+00\n",
      "  -3.49497581e+00  -4.15234613e+00   4.26540804e+00   4.99653864e+00\n",
      "   4.61911964e+00   1.98240614e+00   2.00557470e+00   1.11256113e+01\n",
      "   1.02785647e+00   8.88546848e+00  -6.97745419e+00  -6.39626455e+00\n",
      "   3.98690224e+00  -1.61535892e+01   4.43375540e+00  -6.50761557e+00\n",
      "   7.79895782e-01  -1.82332745e+01   3.45869601e-01  -1.30417604e+01\n",
      "   3.82447028e+00  -2.46498222e+01   7.94510174e+00   4.70394087e+00\n",
      "   1.83431137e+00   1.03649735e+00  -1.15376797e+01   2.67454803e-01\n",
      "  -9.59076691e+00  -1.55748625e+01   1.58001626e+00   6.43529892e+00\n",
      "  -7.53269720e+00   6.21147346e+00  -5.67353964e+00  -3.01669827e+01\n",
      "   3.65746760e+00  -2.32590542e+01   3.14075828e+00  -1.58654428e+00\n",
      "  -3.50731421e+00  -1.24505770e+00   1.67510760e+00   3.60083270e+00\n",
      "   6.43420744e+00  -6.97202301e+00   3.42265773e+00  -1.27744074e+01\n",
      "   4.03264093e+00  -1.99626312e+01   3.24978867e+01   4.02318144e+00\n",
      "   6.03904486e+00   5.22736883e+00   3.10884714e+00  -1.18516855e+01\n",
      "  -2.52527180e+01   5.21104431e+00   2.80228672e+01  -2.53515571e-01\n",
      "  -5.59494925e+00   7.45232439e+00   3.07101130e+00  -9.15553379e+00\n",
      "   3.44615126e+00  -2.17865682e+00   3.03151464e+00   2.15610290e+00\n",
      "  -5.51905632e+00  -4.80496445e+01  -5.10048676e+00   2.17365694e+00\n",
      "  -1.58769333e+00   4.06811237e+00  -1.46137009e+01  -1.33568583e+01\n",
      "   4.40469313e+00   2.96229339e+00  -2.28014870e+01   3.57714963e+00\n",
      "  -1.29970407e+01   3.95015097e+00   4.28751135e+00  -4.18894434e+00\n",
      "   2.23724484e+00   1.53545022e-01  -7.54960012e+00   1.17811596e+00\n",
      "  -2.03299465e+01  -1.64467297e+01  -1.50926905e+01   2.94110990e+00\n",
      "   4.22317696e+00   2.78595304e+00   4.39900732e+00   3.49401683e-01]\n",
      "[  4.11452103e+01   2.31185398e+01   1.11771603e+01   5.16003876e+01\n",
      "   5.30619354e+01   1.29557791e+01   3.03317986e+01   2.61729646e+00\n",
      "   2.30150928e+01   9.17284470e+01   2.62957230e+01   1.40615177e+01\n",
      "   1.00893726e+01   2.23443127e+01   1.37967033e+01   1.07580547e+01\n",
      "   4.25139160e+01   1.08161026e+02   2.98070908e+00   9.05138016e+00\n",
      "   3.10131907e+00   9.22684765e+00   5.28064995e+01   1.59964361e+01\n",
      "   2.41087780e+01   9.74528408e+00   3.91799901e-35   1.60461044e+02\n",
      "   1.16212492e+01   9.64218330e+00   8.55294132e+00   8.61983681e+00\n",
      "   6.61192799e+00   1.16108904e+01   1.64363194e+01   1.02884035e+01\n",
      "   1.55629244e+01   7.72208261e+00   8.94244766e+00   4.35093689e+01\n",
      "   2.57812238e+00   6.88754044e+01   1.86474400e+01   4.17297630e+01\n",
      "   1.80722446e+01   1.65314713e+02   1.13629160e+01   4.37313690e+01\n",
      "   3.71835060e+01   2.01178406e+02   1.74354541e+00   1.17051346e+02\n",
      "   1.50593281e+01   3.25602226e+01   3.74467773e+01   2.33914165e+01\n",
      "   7.08275175e+00   9.03539562e+00   3.24196739e+01   7.15477288e-01\n",
      "   7.01614990e+01   1.10714020e+02   1.10139866e+01   4.78303032e+01\n",
      "   6.04304390e+01   1.70965405e+01   1.29461775e+01   6.86267471e+01\n",
      "   1.08483591e+01   2.16665249e+01   1.64083958e+01   8.29891777e+00\n",
      "   1.35119762e+01   1.02204685e+01   1.25880213e+01   1.89658489e+01\n",
      "   5.66937141e+01   4.04056587e+01   1.76503124e+01   8.47525940e+01\n",
      "   1.62876301e+01   9.79415588e+01   7.21698380e+01   9.34449005e+00\n",
      "   3.68123016e+01   2.90963268e+01   1.22383261e+01   7.81572113e+01\n",
      "   8.71775818e+01   4.76129608e+01   3.98366127e+01   5.40101886e-01\n",
      "   3.59516182e+01   2.99350624e+01   1.82337799e+01   5.13881302e+01\n",
      "   1.17887526e+01   7.67614937e+00   1.42761126e+01   2.06352663e+00\n",
      "   4.49416313e+01   5.84634972e+01   1.68498611e+01   1.60993214e+01\n",
      "   2.93902054e+01   1.19109869e+01   1.05385727e+02   1.14783669e+02\n",
      "   1.40278139e+01   1.57901058e+01   8.50064087e+01   1.81505489e+01\n",
      "   1.01265289e+02   1.32944584e+01   2.53504639e+01   2.10765438e+01\n",
      "   8.41444397e+00   2.43215313e+01   3.96165543e+01   1.41774349e+01\n",
      "   1.20874138e+01   4.78924255e+01   7.98835983e+01   1.38841743e+01\n",
      "   1.50088158e+01   1.49982090e+01   1.48894100e+01   8.15443635e-01]\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "layer26 = model.layers[26]\n",
    "W = np.array(layer26.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[26].get_weights()\n",
    "\n",
    "fMean = open(\"data/EighthLayer/Eighth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/EighthLayer/Eighth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/EighthLayer/Eighth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/EighthLayer/Eighth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Norm for Layer 7 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 3, 32)\n",
      "[ 1.05056036  0.57910722  1.76207137  1.84479833  1.41026509  0.60256934\n",
      "  1.24274731  0.42947298  1.26381314  3.03325653  1.29197574  1.21629047\n",
      "  1.7493304   0.88670272  1.14918911  1.23011827  1.97115421  2.43784595\n",
      "  1.33370745  1.20197749  0.4182142   1.33236325  1.00239277  1.27573729\n",
      "  1.26219618  1.176705    0.18780169  3.93668509  1.7225306   1.46456468\n",
      "  1.0148164   2.50519514  1.46467161  2.33769035  1.39787662  1.60222232\n",
      "  1.8232336   1.30929077  1.5724057   1.22389412  1.12228286  2.14964604\n",
      "  1.71093667  2.03067923  1.07103086  3.16209316  0.95092404  2.5516119\n",
      "  1.43246007  3.33769584  2.34382176  4.09600353  1.89060473  0.69922721\n",
      "  1.99524808  1.93345535  1.25142837  0.83385456  1.25391245  0.86113846\n",
      "  2.35615373  1.80018473  1.02557981  2.05879235  2.70441008  0.82349062\n",
      "  1.70190358  1.34319293  1.53885257  0.63403744  1.53616238  0.75365496\n",
      "  1.1811868   0.98377353  1.22031295  1.72789145  2.66385555  1.59993422\n",
      "  1.13872051  4.04538012  2.0944171   3.30056548  1.3120786   1.21914375\n",
      "  2.34494805  1.83359241  1.19624007  2.49831581  2.18560386  2.5982244\n",
      "  1.0752722   0.18756729  2.12485385  1.45852208  1.25945699  2.3376646\n",
      "  0.87136644  1.31848311  0.95300549  0.43815097  2.57513428  1.63579941\n",
      "  1.73644495  1.63258827  1.24595869  1.74151826  3.44675565  3.69923639\n",
      "  1.25336468  2.0315671   1.94417644  1.02419591  3.45607495  1.77218127\n",
      "  1.58918536  2.03422713  1.75530434  1.2228533   3.2289784   0.78679413\n",
      "  0.60116827  1.49168241  1.91346669  1.37069297  1.0319804   1.53899002\n",
      "  0.6749168   1.96157253]\n",
      "[ 1.37265384  1.9337225  -0.4275859   0.45966831  1.90571523  3.10698295\n",
      "  2.16025639  1.64340281  0.06552611  0.34556508  1.27673697 -0.13176943\n",
      " -0.16817823  0.379803   -0.16727634 -0.18414244 -0.11949538  4.20403242\n",
      " -1.11676192 -0.33154118  1.80451512 -0.31445459  0.82911772  0.06047173\n",
      "  2.45823026 -0.19144356 -2.01320243  1.21578252 -0.50406128 -0.861422\n",
      "  2.0811317  -1.43296885 -0.25472727 -0.67203468  0.01744555 -0.07057729\n",
      " -0.3827537  -0.74370444 -0.95024139  0.78957099 -0.91117501  0.14091317\n",
      "  0.21259372 -0.11106931  0.22444989  1.20935524  2.29332423 -0.38162875\n",
      "  0.96895415  1.88567317 -3.43597364  0.58670568 -0.65941894  2.27478838\n",
      "  0.09890039 -0.51969272 -0.21610326  2.10127711  1.02459264 -0.96349174\n",
      "  0.80302435  1.80490661  0.958799   -0.16800994 -0.2888307   2.22199059\n",
      " -0.14596179  2.91236115 -0.32895011  2.61666608 -0.46688372  1.41919947\n",
      " -0.24241407  0.55335182  0.12249853 -0.38204476 -0.65039057  2.43765044\n",
      " -0.22311573  0.13065745 -0.67969424  1.34610319  3.14788795 -0.18944089\n",
      " -0.45388359 -0.06187332 -0.2685689   0.91854686  2.36902332 -0.88759124\n",
      "  2.4488132   1.83831692 -0.26920208  0.5959022  -0.50750905  0.51064682\n",
      "  0.1895366  -0.70517623 -0.20727065  1.6565181  -0.72306579  5.08148813\n",
      " -0.1415799  -1.01212978  0.97695088 -0.45813024  1.73187804  0.67062575\n",
      " -0.10114212 -0.82642198  2.57383633 -0.15365389  0.63633811 -0.60334283\n",
      " -0.25108281 -0.43604031 -0.79011434  0.51446211 -0.44929591  2.01557589\n",
      "  2.20561123  1.95224404  1.77795124 -0.48286238  0.11404155 -0.61678702\n",
      "  0.43513077 -2.52419019]\n",
      "[ -8.38062227e-01   1.72363460e+00   1.07538521e+00   5.40693521e+00\n",
      "  -2.19357580e-01   2.53496742e+00   1.87434459e+00   3.77432853e-01\n",
      "  -1.34305811e+00  -2.35995531e+00   2.93567252e+00  -1.39303297e-01\n",
      "  -3.20602298e-01   1.22487473e+00   6.93437767e+00   4.84573506e-02\n",
      "  -2.88593245e+00  -4.99837017e+00  -1.73516214e+00   1.25632644e+00\n",
      "   2.35032701e+00  -6.05534792e-01   1.42609799e+00  -3.68939066e+00\n",
      "   4.62253809e+00   5.13590717e+00   5.95235840e-12  -1.12638330e+00\n",
      "  -3.68959993e-01   3.57009590e-01   3.04491907e-01  -2.21810293e+00\n",
      "   3.42683053e+00   2.01885819e+00   2.42712593e+00  -3.94407719e-01\n",
      "   3.49812597e-01   2.00943843e-01   4.78250933e+00  -2.62565327e+00\n",
      "  -2.10399580e+00   1.14639199e+00   4.47692537e+00  -1.40545940e+00\n",
      "  -5.79183772e-02   1.70990527e+00  -3.30736446e+00   3.04026365e-01\n",
      "   1.63905966e+00  -4.48159599e+00  -1.80584446e-01   1.77059519e+00\n",
      "   1.14509666e+00  -5.68514490e+00  -1.69478738e+00   5.20807648e+00\n",
      "   3.50338912e+00   5.27592778e-01   6.24958575e-01  -1.44309437e+00\n",
      "   4.99360487e-02   2.30156803e+00   1.48507670e-01  -2.61059570e+00\n",
      "  -1.98071077e-02  -1.65751278e+00  -1.11545646e+00   8.68363440e-01\n",
      "   1.21068861e-02  -3.28513980e+00   3.75920534e-01  -1.92449260e+00\n",
      "   4.20070410e+00  -1.64712548e+00   6.71229780e-01  -2.14343810e+00\n",
      "  -5.11297047e-01   1.77571583e+00   5.86336279e+00   4.62082326e-02\n",
      "  -6.59788787e-01   8.05086017e-01   2.67099798e-01   5.31867683e-01\n",
      "   2.12555543e-01  -3.86089265e-01   2.24732184e+00   3.81956553e+00\n",
      "   2.97091746e+00  -3.08492088e+00  -1.72885442e+00   3.60219860e+00\n",
      "  -1.85222256e+00   6.97446465e-01   1.21187019e+00   6.12108529e-01\n",
      "   7.93669701e-01   1.54220188e+00   1.94967401e+00  -6.46305978e-01\n",
      "  -7.73514330e-01  -6.56268775e-01  -1.60516632e+00   3.75085163e+00\n",
      "   3.64202231e-01   2.89270592e+00   3.96967947e-01   6.50834203e-01\n",
      "   3.20730877e+00   3.08322406e+00  -1.27607155e+00  -4.15523142e-01\n",
      "  -2.65218830e+00   3.39371413e-01  -1.72026503e+00   4.51901436e-01\n",
      "   4.16413641e+00   1.02270782e+00   1.39290488e+00   2.57016683e+00\n",
      "   2.56167269e+00   2.85741538e-01  -2.23067927e+00  -3.63870168e+00\n",
      "  -2.61048913e+00   1.04774129e+00   3.43152452e+00  -1.03147745e+00]\n",
      "[  9.96715367e-01   1.58896255e+00   2.56640625e+00   1.21486700e+00\n",
      "   1.37680721e+00   1.19716370e+00   1.17963290e+00   8.99641037e-01\n",
      "   7.64323890e-01   1.14802241e+00   1.10909462e+00   1.59556758e+00\n",
      "   2.03418183e+00   1.11261308e+00   1.75489783e+00   1.58398509e+00\n",
      "   1.20647204e+00   9.18176472e-01   1.41453242e+00   1.74353886e+00\n",
      "   6.29960299e-01   1.57618475e+00   2.76562428e+00   1.82785320e+00\n",
      "   4.05374914e-01   1.74723911e+00   3.91628924e-23   2.04270172e+00\n",
      "   1.15283024e+00   8.24670196e-01   9.23377156e-01   3.49753976e+00\n",
      "   1.95072663e+00   1.10316110e+00   2.14506006e+00   2.05354071e+00\n",
      "   2.93755269e+00   1.05969644e+00   9.46161628e-01   2.00973487e+00\n",
      "   1.09282053e+00   1.66543150e+00   2.52487731e+00   1.54499424e+00\n",
      "   1.12595546e+00   1.45830321e+00   6.64857209e-01   1.87220109e+00\n",
      "   7.47650266e-01   2.27403736e+00   8.21067095e-01   1.24311852e+00\n",
      "   2.49208784e+00   9.49720860e-01   3.32553840e+00   3.10620427e+00\n",
      "   9.36835468e-01   9.62598741e-01   1.40628159e+00   7.49689996e-01\n",
      "   1.47244275e+00   9.23238873e-01   5.60583532e-01   1.60926342e+00\n",
      "   1.39678681e+00   1.27483690e+00   3.49079633e+00   1.35214901e+00\n",
      "   1.24790037e+00   1.25891125e+00   2.35427690e+00   6.83549464e-01\n",
      "   1.52630341e+00   9.20882523e-01   5.53385973e-01   2.70208001e+00\n",
      "   1.22069204e+00   5.65000474e-01   1.44587862e+00   1.38327241e+00\n",
      "   1.64558434e+00   1.78969312e+00   1.13359177e+00   1.45684576e+00\n",
      "   1.40880144e+00   1.52125621e+00   8.80076826e-01   1.34121525e+00\n",
      "   1.48328388e+00   2.45492339e+00   1.10290110e+00   6.34244084e-01\n",
      "   1.80560303e+00   1.04103887e+00   1.43701613e+00   1.65173316e+00\n",
      "   1.27426744e+00   1.07670319e+00   1.26876748e+00   1.03438890e+00\n",
      "   1.60988808e+00   1.84967697e+00   1.55145490e+00   1.43088603e+00\n",
      "   9.35320795e-01   2.01729345e+00   1.05425656e+00   1.23297250e+00\n",
      "   1.56704998e+00   3.00755358e+00   1.03336275e+00   1.03721070e+00\n",
      "   1.39556336e+00   2.11952257e+00   1.58265579e+00   1.51623857e+00\n",
      "   1.09678197e+00   5.72463930e-01   1.45521557e+00   7.76128471e-01\n",
      "   1.09751844e+00   1.04238343e+00   9.48275864e-01   1.57336879e+00\n",
      "   1.30721414e+00   1.33693504e+00   6.74388647e-01   7.58969247e-01]\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "layer22 = model.layers[2]\n",
    "W = np.array(layer22.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[22].get_weights()\n",
    "\n",
    "fMean = open(\"data/SeventhLayer/Seventh_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SeventhLayer/Seventh_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SeventhLayer/Seventh_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SeventhLayer/Seventh_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Layer Descriptions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7fe6e3ce8f98>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe6e3cd1550>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e3cd1fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3d2a5c0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3d7d710>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e3d67240>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3b43e80>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3ae7c88>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e3a95ef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3ab1d68>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3a13208>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe6e39e9908>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e39e9978>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e393de10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e393deb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e38e7dd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3887d68>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3820fd0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e37c07f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3783940>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3748278>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fe6e371aef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e36e6fd0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e367fe10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fe6e35e3f28>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fe6e35cdc18>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fe6e3624f60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fe6e3505c50>\n",
      "\n",
      "\n",
      " Weight Size\n",
      "(1, 3, 3, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer25 = model.layers[25]\n",
    "W = np.array(layer25.get_weights())\n",
    "\n",
    "print(\"\\n\\n Weight Size\")\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
