{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Model\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "\n",
    "fileName = \"Trump.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# Global MobileNets model\n",
    "mobile = keras.applications.mobilenet.MobileNet(weights=\"imagenet\")\n",
    "model = Model(mobile.input, mobile.layers[86].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_input(fileName, X):\n",
    "    fhandler = open(fileName, \"w\")\n",
    "    for i in range(3):\n",
    "        for j in range(224):\n",
    "            for k in range(224):\n",
    "                fhandler.write(str(X[j][k][i]) + \"\\n\")\n",
    "    fhandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Input_Files():\n",
    "    \n",
    "    img_width = 224\n",
    "    img_height = 224\n",
    "\n",
    "    img = image.load_img(fileName, target_size=(img_width, img_height))\n",
    "    X = image.img_to_array(img)\n",
    "\n",
    "    # Normalizing the Input\n",
    "    X = tf.keras.utils.normalize(X, axis=2)\n",
    "    \n",
    "    # Write Normalized Input to file\n",
    "    write_to_file_input(\"data/FirstLayer/inputNorm.txt\", X)\n",
    "    \n",
    "    \n",
    "    # Write combination input to file\n",
    "    X[:, :, 0] = 1\n",
    "    X[:, :, 1] = 10\n",
    "    X[:, :, 2] = 100\n",
    "    write_to_file_input(\"data/FirstLayer/inputComb.txt\", X)\n",
    "    \n",
    "    # Write all 1's input to file\n",
    "    X[:, :, 0] = 1\n",
    "    X[:, :, 1] = 1\n",
    "    X[:, :, 2] = 1\n",
    "    write_to_file_input(\"data/FirstLayer/inputSet1.txt\", X)\n",
    "\n",
    "    print(\"Creating Input files Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_Input_Files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_one(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(32):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[2].get_weights()[0]\n",
    "write_to_file_weights_layer_one(\"data/FirstLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_one(\"data/FirstLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_second(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(32):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[5].get_weights()[0]\n",
    "write_to_file_weights_layer_second(\"data/SecondLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_second(\"data/SecondLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkCalculations():\n",
    "\n",
    "    img_width = 224\n",
    "    img_height = 224\n",
    "\n",
    "    img = image.load_img(fileName, target_size=(img_width, img_height))\n",
    "    X = image.img_to_array(img)\n",
    "\n",
    "    # Normalized the Input\n",
    "    X = tf.keras.utils.normalize(X, axis=2)\n",
    "    \n",
    "    # Set as 1\n",
    "    #X[:,:,0] = 1\n",
    "    #X[:,:,1] = 1\n",
    "    #X[:,:,2] = 1\n",
    "\n",
    "    # Set as Combination\n",
    "    #X[:,:,0] = 1\n",
    "    #X[:,:,1] = 10\n",
    "    #X[:,:,2] = 100\n",
    "\n",
    "    print(X[0][0][0])\n",
    "    print(X[0][1][0])\n",
    "    print(X[0][2][0])\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    print(X.shape)\n",
    "\n",
    "    \"\"\"\n",
    "    # Weights part\n",
    "    W = model.layers[2].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[2].set_weights(W)\n",
    "    \n",
    "    \n",
    "    W = model.layers[5].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[5].set_weights(W)\n",
    "    \n",
    "    W = model.layers[8].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[8].set_weights(W)\n",
    "    \n",
    "    W = model.layers[12].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[12].set_weights(W)\n",
    "    \n",
    "    W = model.layers[15].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[15].set_weights(W)\n",
    "    \n",
    "    W = model.layers[18].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[18].set_weights(W)\n",
    "    \n",
    "    W = model.layers[21].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[21].set_weights(W)\n",
    "    \n",
    "    W = model.layers[25].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[25].set_weights(W)\n",
    "    \n",
    "    W = model.layers[28].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[28].set_weights(W)\n",
    "    W = model.layers[28].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[31].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[31].set_weights(W)\n",
    "    W = model.layers[31].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[34].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[34].set_weights(W)\n",
    "    W = model.layers[34].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[38].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[38].set_weights(W)\n",
    "    W = model.layers[38].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[41].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[41].set_weights(W)\n",
    "    W = model.layers[41].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[44].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[44].set_weights(W)\n",
    "    W = model.layers[44].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[47].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[47].set_weights(W)\n",
    "    W = model.layers[47].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[50].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[50].set_weights(W)\n",
    "    W = model.layers[50].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[53].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[53].set_weights(W)\n",
    "    W = model.layers[53].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[56].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[56].set_weights(W)\n",
    "    W = model.layers[56].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[59].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[59].set_weights(W)\n",
    "    W = model.layers[59].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[62].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[62].set_weights(W)\n",
    "    W = model.layers[62].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[65].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[65].set_weights(W)\n",
    "    W = model.layers[65].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[68].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[68].set_weights(W)\n",
    "    W = model.layers[68].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[71].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[71].set_weights(W)\n",
    "    W = model.layers[71].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[75].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[75].set_weights(W)\n",
    "    W = model.layers[75].get_weights()[0]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    output = model.predict(X)\n",
    "    print(\"Output Shape ---> \")\n",
    "    print(output.shape)\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    # Save Output to file\n",
    "    fOutput = open(\"data/TwentySevenLayer/TwentySeven_Layer_Output.txt\", \"w\")\n",
    "    for i in range(len(output)):\n",
    "        for j in range(len(output[0][0][0])):\n",
    "            for k in range(len(output[0])):\n",
    "                for l in range(len(output[0][0])):\n",
    "                    fOutput.write(str(output[i][k][l][j]) + \"\\n\")\n",
    "    fOutput.close()\n",
    "    print(\"Output File Writing Complete!!!\")\n",
    "\n",
    "    print(\"\\n\\n Description of Layers\")\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time --> 1550088573.8246968\n",
      "0.14501739\n",
      "0.35173547\n",
      "0.5635475\n",
      "(1, 224, 224, 3)\n",
      "Output Shape ---> \n",
      "(1, 7, 7, 1024)\n",
      "--------------------------------\n",
      "Output File Writing Complete!!!\n",
      "\n",
      "\n",
      " Description of Layers\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f922d02aef0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f919f12e0f0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f919f0f73c8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919ef76f28>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919ef76dd8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f919e73a9b0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c6b2fd0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c60c4a8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f919c6292e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c59d278>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c607860>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f919c586ac8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f919c53d208>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c476518>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c476748>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f919c415828>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c3d40b8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c3d4a20>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f919c353da0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c332940>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c2a2cf8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f919c255ac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c1d1f60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c1f8dd8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f919c1b4b70>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f919c171be0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c0f0dd8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c0f0e80>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f919c0926a0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f919c053908>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f919c029a90>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f918c7c18d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c7cfeb8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c774668>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f918c71f5f8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c691438>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c679a20>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f918c5ffc88>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f918c61cf60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c64ed30>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c5d8c50>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f918c5127f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c4cc9e8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c44de48>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f918c46e978>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c42d9e8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c399f98>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f918c34ec50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c308ba8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c2efba8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f918c28b160>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c26a4a8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c1cf400>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f918c1e86a0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c1600b8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f918c147780>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f918c0caa90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f918c122860>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f91787bd860>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f91787ea780>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f9178756780>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f9178745be0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f91786cbef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f91786b7780>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f9178628be0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f91785d99b0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f9178598ba8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f91785080b8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f9178538b38>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f91784f4208>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f917845d1d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f9178476400>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f91783d6e48>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f9178339d68>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f9178354320>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f9178378550>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f917824b320>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f917824b550>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f9178271630>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f9178229860>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f917824b588>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f917819bfd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f91780cdda0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f91780f0fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f917808b048>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f917806bf60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f9178099f28>\n",
      "Total Time --> 5.345784664154053\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(\"Total Time -->\", (start))\n",
    "checkCalculations()\n",
    "end = time.time()\n",
    "print(\"Total Time -->\", (end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 8 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_EighthLayer():\n",
    "    fExp = open('data/EighthLayer/Eighth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/EighthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    mismatchStart = 0;\n",
    "    boo = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            if(boo < 500):\n",
    "                boo += 1\n",
    "                print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) + \" <---- \" + str(i) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    print(mismatchStart)\n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_EighthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 6 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SixthLayer():\n",
    "    fExp = open('data/SixthLayer/Sixth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SixthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    mismatchStart = 0;\n",
    "    boo = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            if(boo < 500):\n",
    "                boo += 1\n",
    "                print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) + \" <---- \" + str(i) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    print(mismatchStart)\n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_SixthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 5 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FifthLayer():\n",
    "    fExp = open('data/FifthLayer/Fifth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FifthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_FifthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 1 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FirstLayer():\n",
    "    fExp = open('data/FirstLayer/First_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FirstLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_FirstLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 4 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FourthLayer():\n",
    "    fExp = open('data/FourthLayer/Fourth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FourthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_FourthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SecondLayer():\n",
    "    fExp = open('data/SecondLayer/Second_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SecondLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFile_SecondLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_ThirdLayer():\n",
    "    fExp = open('data/ThirdLayer/Third_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/ThirdLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 7 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SeventhLayer():\n",
    "    fExp = open('data/SeventhLayer/Seventh_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SeventhLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFile_SeventhLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 6 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[19].get_weights()\n",
    "\n",
    "fMean = open(\"data/SixthLayer/Sixth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SixthLayer/Sixth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SixthLayer/Sixth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SixthLayer/Sixth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[6].get_weights()\n",
    "\n",
    "fMean = open(\"data/SecondLayer/Second_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SecondLayer/Second_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SecondLayer/Second_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SecondLayer/Second_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 3  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_three(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(64):\n",
    "                for l in range(32):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer8 = model.layers[8]\n",
    "W = np.array(layer8.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer8.get_weights()[0]\n",
    "write_to_file_weights_layer_three(\"data/ThirdLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_three(\"data/ThirdLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 3 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[9].get_weights()\n",
    "\n",
    "fMean = open(\"data/ThirdLayer/Third_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/ThirdLayer/Third_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/ThirdLayer/Third_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/ThirdLayer/Third_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 4  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_four(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(64):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer12 = model.layers[12]\n",
    "W = np.array(layer12.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer12.get_weights()[0]\n",
    "write_to_file_weights_layer_four(\"data/FourthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_four(\"data/FourthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 5 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer16 = model.layers[16]\n",
    "W = np.array(layer16.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[16].get_weights()\n",
    "\n",
    "fMean = open(\"data/FifthLayer/Fifth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/FifthLayer/Fifth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/FifthLayer/Fifth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/FifthLayer/Fifth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 4 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer13 = model.layers[13]\n",
    "W = np.array(layer13.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "fMean = open(\"data/FourthLayer/Fourth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/FourthLayer/Fourth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/FourthLayer/Fourth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/FourthLayer/Fourth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 5  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_five(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(128):\n",
    "            for k in range(1):\n",
    "                for l in range(64):\n",
    "                    fTemp.write(str(W[i][k][l][j]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer15 = model.layers[15]\n",
    "W = np.array(layer15.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer15.get_weights()[0]\n",
    "write_to_file_weights_layer_five(\"data/FifthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_five(\"data/FifthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 6  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_six(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(128):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(1):\n",
    "                    fTemp.write(str(W[j][k][i][l]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer18 = model.layers[18]\n",
    "W = np.array(layer18.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer18.get_weights()[0]\n",
    "write_to_file_weights_layer_six(\"data/SixthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_six(\"data/SixthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 7  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_seven(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(128):\n",
    "                for l in range(128):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer21 = model.layers[21]\n",
    "W = np.array(layer21.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer21.get_weights()[0]\n",
    "write_to_file_weights_layer_seven(\"data/SeventhLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_seven(\"data/SeventhLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 8  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_eight(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(128):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(1):\n",
    "                    fTemp.write(str(W[j][k][i][l]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer25 = model.layers[25]\n",
    "W = np.array(layer25.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer25.get_weights()[0]\n",
    "write_to_file_weights_layer_eight(\"data/EighthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_eight(\"data/EighthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Norm for Layer 8 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer26 = model.layers[26]\n",
    "W = np.array(layer26.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[26].get_weights()\n",
    "\n",
    "fMean = open(\"data/EighthLayer/Eighth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/EighthLayer/Eighth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/EighthLayer/Eighth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/EighthLayer/Eighth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Norm for Layer 7 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer22 = model.layers[2]\n",
    "W = np.array(layer22.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[22].get_weights()\n",
    "\n",
    "fMean = open(\"data/SeventhLayer/Seventh_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SeventhLayer/Seventh_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SeventhLayer/Seventh_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SeventhLayer/Seventh_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Layer Descriptions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer25 = model.layers[25]\n",
    "W = np.array(layer25.get_weights())\n",
    "\n",
    "print(\"\\n\\n Weight Size\")\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
