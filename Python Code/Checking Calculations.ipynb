{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Model\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "\n",
    "fileName = \"Trump.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# Global MobileNets model\n",
    "mobile = keras.applications.mobilenet.MobileNet(weights=\"imagenet\")\n",
    "model = Model(mobile.input, mobile.layers[92].output)\n",
    "print(len(mobile.layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_input(fileName, X):\n",
    "    fhandler = open(fileName, \"w\")\n",
    "    for i in range(3):\n",
    "        for j in range(224):\n",
    "            for k in range(224):\n",
    "                fhandler.write(str(X[j][k][i]) + \"\\n\")\n",
    "    fhandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Input_Files():\n",
    "    \n",
    "    img_width = 224\n",
    "    img_height = 224\n",
    "\n",
    "    img = image.load_img(fileName, target_size=(img_width, img_height))\n",
    "    X = image.img_to_array(img)\n",
    "\n",
    "    # Normalizing the Input\n",
    "    X = tf.keras.utils.normalize(X, axis=2)\n",
    "    \n",
    "    # Write Normalized Input to file\n",
    "    write_to_file_input(\"data/FirstLayer/inputNorm.txt\", X)\n",
    "    \n",
    "    \n",
    "    # Write combination input to file\n",
    "    X[:, :, 0] = 1\n",
    "    X[:, :, 1] = 10\n",
    "    X[:, :, 2] = 100\n",
    "    write_to_file_input(\"data/FirstLayer/inputComb.txt\", X)\n",
    "    \n",
    "    # Write all 1's input to file\n",
    "    X[:, :, 0] = 1\n",
    "    X[:, :, 1] = 1\n",
    "    X[:, :, 2] = 1\n",
    "    write_to_file_input(\"data/FirstLayer/inputSet1.txt\", X)\n",
    "\n",
    "    print(\"Creating Input files Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_Input_Files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_one(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(32):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[2].get_weights()[0]\n",
    "write_to_file_weights_layer_one(\"data/FirstLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_one(\"data/FirstLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_second(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(32):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[5].get_weights()[0]\n",
    "write_to_file_weights_layer_second(\"data/SecondLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_second(\"data/SecondLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkCalculations():\n",
    "\n",
    "    img_width = 224\n",
    "    img_height = 224\n",
    "\n",
    "    img = image.load_img(fileName, target_size=(img_width, img_height))\n",
    "    X = image.img_to_array(img)\n",
    "\n",
    "    # Normalized the Input\n",
    "    X = tf.keras.utils.normalize(X, axis=2)\n",
    "    \n",
    "    # Set as 1\n",
    "    #X[:,:,0] = 1\n",
    "    #X[:,:,1] = 1\n",
    "    #X[:,:,2] = 1\n",
    "\n",
    "    # Set as Combination\n",
    "    #X[:,:,0] = 1\n",
    "    #X[:,:,1] = 10\n",
    "    #X[:,:,2] = 100\n",
    "\n",
    "    print(X[0][0][0])\n",
    "    print(X[0][1][0])\n",
    "    print(X[0][2][0])\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    print(X.shape)\n",
    "\n",
    "    \"\"\"\n",
    "    # Weights part\n",
    "    W = model.layers[2].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[2].set_weights(W)\n",
    "    \n",
    "    \n",
    "    W = model.layers[5].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[5].set_weights(W)\n",
    "    \n",
    "    W = model.layers[8].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[8].set_weights(W)\n",
    "    \n",
    "    W = model.layers[12].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[12].set_weights(W)\n",
    "    \n",
    "    W = model.layers[15].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[15].set_weights(W)\n",
    "    \n",
    "    W = model.layers[18].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[18].set_weights(W)\n",
    "    \n",
    "    W = model.layers[21].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[21].set_weights(W)\n",
    "    \n",
    "    W = model.layers[25].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[25].set_weights(W)\n",
    "    \n",
    "    W = model.layers[28].get_weights()[0]\n",
    "    # Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[28].set_weights(W)\n",
    "    W = model.layers[28].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[31].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[31].set_weights(W)\n",
    "    W = model.layers[31].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[34].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[34].set_weights(W)\n",
    "    W = model.layers[34].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[38].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[38].set_weights(W)\n",
    "    W = model.layers[38].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[41].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[41].set_weights(W)\n",
    "    W = model.layers[41].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[44].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[44].set_weights(W)\n",
    "    W = model.layers[44].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[47].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[47].set_weights(W)\n",
    "    W = model.layers[47].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[50].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[50].set_weights(W)\n",
    "    W = model.layers[50].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[53].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[53].set_weights(W)\n",
    "    W = model.layers[53].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[56].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[56].set_weights(W)\n",
    "    W = model.layers[56].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[59].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[59].set_weights(W)\n",
    "    W = model.layers[59].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[62].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[62].set_weights(W)\n",
    "    W = model.layers[62].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[65].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[65].set_weights(W)\n",
    "    W = model.layers[65].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[68].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[68].set_weights(W)\n",
    "    W = model.layers[68].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[71].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[71].set_weights(W)\n",
    "    W = model.layers[71].get_weights()[0]\n",
    "    \n",
    "    W = model.layers[75].get_weights()[0]\n",
    "    #Weights set as 1\n",
    "    W[:,:,:,:] = 1\n",
    "    W = np.expand_dims(W, axis=0)\n",
    "    model.layers[75].set_weights(W)\n",
    "    W = model.layers[75].get_weights()[0]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    output = model.predict(X)\n",
    "    print(\"Output Shape ---> \")\n",
    "    print(output.shape)\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    # Save Output to file\n",
    "    fOutput = open(\"data/TwentyNineLayer/TwentyNine_Layer_Output.txt\", \"w\")\n",
    "    for i in range(len(output)):\n",
    "        for j in range(len(output[0])):\n",
    "            fOutput.write(str(output[i][j]) + \"\\n\")\n",
    "    fOutput.close()\n",
    "    print(\"Output File Writing Complete!!!\")\n",
    "\n",
    "    print(\"\\n\\n Description of Layers\")\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time --> 1550120853.4857998\n",
      "0.14501739\n",
      "0.35173547\n",
      "0.5635475\n",
      "(1, 224, 224, 3)\n",
      "Output Shape ---> \n",
      "(1, 1000)\n",
      "--------------------------------\n",
      "Output File Writing Complete!!!\n",
      "\n",
      "\n",
      " Description of Layers\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f5860197860>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f57d0a3c550>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57d08ae080>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57d00730b8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57d0073eb8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57d008ec50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c87e0780>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c87b8630>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c8758470>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c86ca400>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c87329e8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f57c86b4c50>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c8652f28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c85a56a0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c85a58d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c85429b0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c84bff28>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c84eeda0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c84a1a58>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c8461128>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c83c4f98>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c8383c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c82fbe10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c8324ac8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f57c82e2cf8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c829fd68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c821ef60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c821eeb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c81d7208>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c8195320>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c8180908>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c8100b70>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c8153898>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c80b07f0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c8060780>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c078e5c0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c077aba8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f57c06ffe10>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c07325f8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57d0051e10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57d0051160>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c0610978>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c05ceb70>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c053f0f0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c056eb00>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c052c1d0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c0490198>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c043cdd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c040ae10>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c03edd30>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c03872e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c03695f8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c02cb588>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c02e9828>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c025e4a8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c0247908>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57c01c9c18>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c021f9e8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57c00fa9e8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57c0129908>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57c009a908>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a87d7d30>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57a877a860>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a87b6908>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a8725d68>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57a86d9b38>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a8693d30>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a85fffd0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57a85b9cc0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a85f5358>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a855b358>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57a8577588>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a84d7f98>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a84b6ef0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f57a84554a8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57a84756d8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a834d4a8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a834d6d8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57a836e7b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a83289e8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a82abe10>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f57a824a940>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a82ece80>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a81efba8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57a818c1d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f57a8105278>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f57a816c6d8>\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7f57a80f1e10>\n",
      "<keras.layers.core.Reshape object at 0x7f57a8047cc0>\n",
      "<keras.layers.core.Dropout object at 0x7f57a8047c88>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f57a805f828>\n",
      "<keras.layers.core.Activation object at 0x7f57927b4160>\n",
      "<keras.layers.core.Reshape object at 0x7f57927b4240>\n",
      "Total Time --> 0.039818525314331055\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(\"Total Time -->\", (start))\n",
    "checkCalculations()\n",
    "end = time.time()\n",
    "print(\"Total Time -->\", (end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 8 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_EighthLayer():\n",
    "    fExp = open('data/EighthLayer/Eighth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/EighthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    mismatchStart = 0;\n",
    "    boo = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            if(boo < 500):\n",
    "                boo += 1\n",
    "                print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) + \" <---- \" + str(i) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    print(mismatchStart)\n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_EighthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 6 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SixthLayer():\n",
    "    fExp = open('data/SixthLayer/Sixth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SixthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    mismatchStart = 0;\n",
    "    boo = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            if(boo < 500):\n",
    "                boo += 1\n",
    "                print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) + \" <---- \" + str(i) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    print(mismatchStart)\n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_SixthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 5 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FifthLayer():\n",
    "    fExp = open('data/FifthLayer/Fifth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FifthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"  --->   \" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_FifthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 1 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FirstLayer():\n",
    "    fExp = open('data/FirstLayer/First_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FirstLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_FirstLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 4 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_FourthLayer():\n",
    "    fExp = open('data/FourthLayer/Fourth_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/FourthLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkFile_FourthLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SecondLayer():\n",
    "    fExp = open('data/SecondLayer/Second_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SecondLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFile_SecondLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_ThirdLayer():\n",
    "    fExp = open('data/ThirdLayer/Third_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/ThirdLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Checking calculations for Layer 7 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_SeventhLayer():\n",
    "    fExp = open('data/SeventhLayer/Seventh_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/SeventhLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()) )\n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFile_SeventhLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 6 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[19].get_weights()\n",
    "\n",
    "fMean = open(\"data/SixthLayer/Sixth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SixthLayer/Sixth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SixthLayer/Sixth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SixthLayer/Sixth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[6].get_weights()\n",
    "\n",
    "fMean = open(\"data/SecondLayer/Second_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SecondLayer/Second_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SecondLayer/Second_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SecondLayer/Second_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 3  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_three(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(64):\n",
    "                for l in range(32):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer8 = model.layers[8]\n",
    "W = np.array(layer8.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer8.get_weights()[0]\n",
    "write_to_file_weights_layer_three(\"data/ThirdLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_three(\"data/ThirdLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 3 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "batchNorm = model.layers[9].get_weights()\n",
    "\n",
    "fMean = open(\"data/ThirdLayer/Third_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/ThirdLayer/Third_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/ThirdLayer/Third_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/ThirdLayer/Third_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 4  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_four(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(64):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    fTemp.write(str(W[k][l][j][i]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer12 = model.layers[12]\n",
    "W = np.array(layer12.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer12.get_weights()[0]\n",
    "write_to_file_weights_layer_four(\"data/FourthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_four(\"data/FourthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 5 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer16 = model.layers[16]\n",
    "W = np.array(layer16.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[16].get_weights()\n",
    "\n",
    "fMean = open(\"data/FifthLayer/Fifth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/FifthLayer/Fifth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/FifthLayer/Fifth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/FifthLayer/Fifth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Batch Norm for Layer 4 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer13 = model.layers[13]\n",
    "W = np.array(layer13.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "fMean = open(\"data/FourthLayer/Fourth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/FourthLayer/Fourth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/FourthLayer/Fourth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/FourthLayer/Fourth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 5  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_five(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(128):\n",
    "            for k in range(1):\n",
    "                for l in range(64):\n",
    "                    fTemp.write(str(W[i][k][l][j]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer15 = model.layers[15]\n",
    "W = np.array(layer15.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer15.get_weights()[0]\n",
    "write_to_file_weights_layer_five(\"data/FifthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_five(\"data/FifthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 6  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_six(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(128):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(1):\n",
    "                    fTemp.write(str(W[j][k][i][l]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer18 = model.layers[18]\n",
    "W = np.array(layer18.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer18.get_weights()[0]\n",
    "write_to_file_weights_layer_six(\"data/SixthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_six(\"data/SixthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 7  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_seven(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(128):\n",
    "                for l in range(128):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer21 = model.layers[21]\n",
    "W = np.array(layer21.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer21.get_weights()[0]\n",
    "write_to_file_weights_layer_seven(\"data/SeventhLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_seven(\"data/SeventhLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving Weights for Layer 8  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_eight(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(128):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(1):\n",
    "                    fTemp.write(str(W[j][k][i][l]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer25 = model.layers[25]\n",
    "W = np.array(layer25.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer25.get_weights()[0]\n",
    "write_to_file_weights_layer_eight(\"data/EighthLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_eight(\"data/EighthLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Norm for Layer 8 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer26 = model.layers[26]\n",
    "W = np.array(layer26.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[26].get_weights()\n",
    "\n",
    "fMean = open(\"data/EighthLayer/Eighth_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/EighthLayer/Eighth_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/EighthLayer/Eighth_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/EighthLayer/Eighth_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Norm for Layer 7 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "layer22 = model.layers[2]\n",
    "W = np.array(layer22.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[22].get_weights()\n",
    "\n",
    "fMean = open(\"data/SeventhLayer/Seventh_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/SeventhLayer/Seventh_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/SeventhLayer/Seventh_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/SeventhLayer/Seventh_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Layer Descriptions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer25 = model.layers[25]\n",
    "W = np.array(layer25.get_weights())\n",
    "\n",
    "print(\"\\n\\n Weight Size\")\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
