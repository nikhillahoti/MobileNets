{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Model\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv3D\n",
    "\n",
    "fileName = \"Trump.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# Global MobileNets model\n",
    "mobile = keras.applications.mobilenet.MobileNet(weights=\"imagenet\")\n",
    "model = Model(mobile.input, mobile.layers[37].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7fdcb8251ac8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fdc29a23128>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fdc29846dd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc29873e48>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc29873fd0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fdc29001d30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc287a9cf8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc287093c8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fdc28724208>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc286ff908>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc28656a90>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fdc2867d8d0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fdc28720748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc2856f438>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc2856f668>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fdc28510748>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc284cf198>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc284cf940>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fdc2844dcc0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc2842a860>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc2839ac18>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fdc2834d9e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc282ccf60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc282fa0b8>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fdc282aca90>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fdc2826bb00>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc281ebcf8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc281ebf28>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fdc281ebf60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc2814df60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc2812be80>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7fdc280c9438>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc280a9748>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc2807c588>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fdc2802a898>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fdc20792358>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7fdc2077e940>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7fdc206ffba8>\n",
      "\n",
      "\n",
      " Weight Size\n",
      "(1, 1, 1, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer34 = model.layers[34]\n",
    "W = np.array(layer34.get_weights())\n",
    "\n",
    "print(\"\\n\\n Weight Size\")\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file_weights_layer_Eleven(fileName, W):\n",
    "    fTemp = open(fileName, \"w\")\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            for k in range(256):\n",
    "                for l in range(256):\n",
    "                    fTemp.write(str(W[i][j][l][k]) + \"\\n\")\n",
    "    fTemp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f7844e856a0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f77b6f6e160>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f77b6d9d0b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b6562080>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b6562f28>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f77b65169e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b44effd0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b44ca668>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f77b446c4a8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b43dd438>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b4445a20>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f77b43c7c88>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f77b4365f60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b42b96d8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b42b9908>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f77b42549e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b41d3f60>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b42030b8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f77b41b5a90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b4174160>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b40d7fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f77b4097c88>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77b400ee48>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77b4038b00>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f77ac7cf828>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f77ac7e1d30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77ac720f98>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77ac720fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f77ac6d8240>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77ac697358>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77ac682940>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f77ac600ba8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77ac6548d0>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77ac5b1828>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f77ac5617b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f77ac4cf5f8>\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f77ac4b9be0>\n",
      "(1, 1, 1, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    \n",
    "layer34 = model.layers[34]\n",
    "W = np.array(layer34.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "W = layer34.get_weights()[0]\n",
    "write_to_file_weights_layer_Eleven(\"data/EleventhLayer/weightsNorm.txt\", W)\n",
    "\n",
    "W[:,:,:,:] = 1\n",
    "write_to_file_weights_layer_Eleven(\"data/EleventhLayer/weightsSet1.txt\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256)\n",
      "[1.4743186  0.7849149  1.0635685  1.0144806  0.5440517  1.5371735\n",
      " 1.5725136  1.2232534  0.61456716 0.754606   1.125535   1.6149791\n",
      " 1.8465903  1.609235   1.470266   2.2028067  0.63144743 1.499836\n",
      " 1.5703726  1.1589489  1.4842225  1.0344683  0.80925024 1.384686\n",
      " 1.3786509  1.22561    0.8812791  1.17811    1.3819901  0.7747353\n",
      " 1.470029   1.3136806  1.5028018  0.63191366 1.2789258  1.0615292\n",
      " 1.1312134  1.3256764  1.0533019  1.3126796  1.1084086  0.76505744\n",
      " 1.4596598  1.3914571  1.2597972  0.66307133 1.4596121  0.68823004\n",
      " 0.8191295  1.6120989  1.8448745  1.1257927  1.4297749  1.2904788\n",
      " 1.7554001  1.5270028  1.8810902  1.644811   0.79412794 1.4249711\n",
      " 1.2593206  1.4234602  1.356936   1.3114222  1.5233407  0.72937965\n",
      " 1.5450163  0.9090805  1.2753205  2.2425625  1.4535905  0.80157936\n",
      " 0.8689489  1.5380288  0.6203756  2.0308754  1.4857674  1.3512836\n",
      " 0.70078313 1.3289853  1.512274   1.2840763  1.6875838  1.4890366\n",
      " 1.5011452  0.45745504 0.8367155  2.0586088  1.9369776  1.3983052\n",
      " 1.5422989  1.4591417  1.5273598  0.52112466 0.84072775 1.5091199\n",
      " 1.1984917  0.7524891  2.161489   0.8810593  1.164584   1.159604\n",
      " 1.2882863  0.44300467 0.9820642  1.6149056  1.3596554  0.626662\n",
      " 0.77483934 0.6312015  0.87367636 1.3574811  1.3092859  1.2934602\n",
      " 1.4185485  1.0543919  2.0123692  1.4244916  1.3228711  1.2423488\n",
      " 0.634086   1.3972191  1.979035   1.7097716  0.86444896 1.4182624\n",
      " 1.598425   0.5970733  0.7351404  2.1312923  1.3766863  1.5122101\n",
      " 1.6509902  1.3571404  0.75454533 1.6149374  1.244839   0.57658386\n",
      " 1.4016112  0.7188333  0.32980147 1.4757185  0.9371714  0.81509835\n",
      " 0.6565139  1.4388261  1.4149308  1.7541033  1.6777117  1.6560915\n",
      " 1.4224828  1.2377937  0.7803413  0.642011   3.0875428  1.7348565\n",
      " 1.2699748  1.3942192  1.4177835  1.6495663  1.4521382  1.2620947\n",
      " 1.2187638  2.0641446  0.86115456 1.7951053  0.86549455 1.4868118\n",
      " 1.4010761  0.717838   1.4024323  1.3267746  0.524334   1.1578832\n",
      " 0.5310034  1.2815229  0.9656648  1.3731679  1.3040885  1.2626444\n",
      " 1.3012806  0.92352754 1.1352346  1.2847323  1.2760801  1.1527058\n",
      " 1.2853453  1.3100275  1.899528   1.5029327  1.8549274  1.3302286\n",
      " 1.2249418  1.2827678  1.278067   1.3588338  1.5578033  0.5456999\n",
      " 1.2779788  1.2864813  1.4877374  0.8277945  1.553139   1.5533818\n",
      " 0.568641   1.2103521  1.815062   0.33724546 0.34892854 1.2282387\n",
      " 1.6841689  0.7699755  1.3431413  1.4536762  1.9167899  1.4692996\n",
      " 1.0598973  1.1649954  1.324018   1.9637594  1.0313591  1.7780005\n",
      " 2.7245078  1.2730347  1.5785779  1.3731991  0.47598132 0.95979136\n",
      " 2.893267   1.4448886  0.5956946  1.0733691  1.3630053  1.1894935\n",
      " 1.31944    2.1309826  1.5402695  1.1914382  1.3677295  1.3798429\n",
      " 1.3860066  1.9447151  1.3554022  1.5710586  1.4054092  0.6390741\n",
      " 1.2101434  1.0016139  1.5630691  1.6287224  1.5422871  0.6541072\n",
      " 1.4023293  1.6052814  1.8863298  1.8482921 ]\n",
      "[-2.3764513   0.96817136 -1.3748016   1.8810191   1.9270185  -0.7582432\n",
      " -0.2684901  -2.102886    1.2949437  -0.55953276  0.64630735 -0.01593609\n",
      "  1.1706458  -0.7251497   0.53617924 -0.5495562   1.733332    0.23677182\n",
      " -0.05061314 -1.0003865  -0.9654198   1.8644322   0.9730174   0.10138871\n",
      " -0.09616251 -0.20600265  1.20471    -0.02331088 -0.30532655  1.4987048\n",
      " -1.1739994   1.0570636  -0.5259407   2.0617225  -1.0622824   1.9192582\n",
      " -1.2540181  -0.37872496  1.2171555  -0.833959   -0.94725573  2.1462643\n",
      "  0.04998762 -0.5450793  -0.49596843  2.3035731  -0.95035124  1.5331912\n",
      "  0.2180627   0.1175252  -0.52591395  0.6371833   0.79441124 -0.3541978\n",
      " -0.9239081  -0.84883237  0.7212437  -0.6280331   0.7874484  -0.6014105\n",
      "  1.5976416  -0.8325181  -0.742686   -1.030312   -0.42836958  0.9237823\n",
      "  0.21443267  0.6629276  -0.8216134  -1.1529148  -0.92039603  1.0231733\n",
      "  1.0111759  -0.45574898  2.1650226   2.0178945   1.5253602  -1.4593953\n",
      "  1.399178    0.7027477  -0.84766865 -1.1298674  -1.3489181   0.00915729\n",
      "  3.4599938   2.02931     1.012678    1.5507226  -0.35650098 -0.4156228\n",
      " -0.27444807 -0.73773885 -0.9964027   1.9703798   2.5128305   0.6606771\n",
      "  1.6788756   1.8801808  -0.6538271   1.6832867   0.04544741  1.7344788\n",
      " -0.36196396  1.231322    2.0746658  -1.1621348  -0.08822664  1.8660738\n",
      "  1.3359678   2.1588795   1.779828   -1.4324186  -1.2629552  -1.328261\n",
      "  1.0524939  -0.6675533   2.4698672  -0.6996698  -1.2874347   0.09311507\n",
      "  1.0222715   0.1876335   0.7327256   0.9830924   1.867955   -0.8726614\n",
      " -0.716699    1.8538079   2.2685113  -1.4263747  -0.500194   -0.9148741\n",
      " -0.03784314 -0.6111045   1.1580405   0.5298276  -1.0192283   1.5525786\n",
      "  0.35257176  1.9083967   1.345841    1.2025456  -0.11238082  2.014915\n",
      "  2.3860118   0.03026166 -1.0530239  -1.0196207  -0.37029362 -0.5933059\n",
      "  1.3084568   0.04582     2.151121    2.119726   -0.3627821   1.3273338\n",
      " -0.8934123  -0.45849046 -0.8891951  -1.0839645  -1.5736078  -0.6004501\n",
      "  0.45452088 -1.1283334   2.3405166  -0.5021674   2.1156259   0.10469956\n",
      "  0.8260591   1.9039357  -0.96915776  1.2312793   1.5567293  -1.4275695\n",
      "  1.8559548  -0.06429843  1.1447477  -1.4959385  -0.5894693   0.9005711\n",
      " -0.331679    2.1341496   1.2349536   0.24081467  1.4334432   1.9330986\n",
      "  1.2969795  -0.19036107 -0.74586624 -0.46328565 -1.5415175  -0.3681009\n",
      "  0.31126016  0.09812346  0.6529624   0.73408866 -0.9483079   1.4854205\n",
      " -0.5019789   0.89298445  0.75415987  1.7931837  -0.73797965  1.9396932\n",
      "  2.0983877   0.6827663  -0.74473184  1.4033957   1.1955544  -0.3814567\n",
      " -0.27405548  1.5893023  -0.9478617   1.2199943   0.53970397 -0.39161536\n",
      " -1.0937651   0.7021725   0.12304167 -0.876355   -0.171337    1.2421788\n",
      "  0.25329718 -0.13984604 -0.703275   -1.2676747   1.6981766  -1.0673748\n",
      " -0.49593297 -0.254008    1.6651008  -0.56175935 -0.2751695  -0.49610412\n",
      " -0.831843   -0.3094946  -0.93324345 -0.41394168 -0.99773306 -1.1168976\n",
      " -0.34119678  1.3432012  -0.96082616 -0.49745283 -0.8163855   1.4203714\n",
      "  1.1837969  -0.15810956 -1.394837    0.10160856  1.8174702   1.7964417\n",
      " -0.93597966 -0.671261    0.6063867   0.29148751]\n",
      "[-2.7200458   1.3308544  -1.1757313   1.0332001  -1.4418664  -0.66499436\n",
      " -0.16987304 -1.6780205   0.69406164  1.1958089  -0.38522133 -0.99705154\n",
      "  0.21193154 -0.868316   -0.3024301  -0.43928713 -0.5254892  -2.191571\n",
      "  0.28177002 -0.8567631  -1.1022856   0.32447347  0.47252005  2.3039708\n",
      " -2.321971   -1.036921    0.02106598 -0.07098547  0.26611674 -0.0808185\n",
      "  0.17641987  0.9886831   0.7695487   2.6519692  -0.41311756  1.1003239\n",
      "  0.96021044  0.73455316  0.651424    0.2808004  -0.99218726  1.7233102\n",
      " -0.9309133  -0.21083927  1.9979978  -0.855906    0.5374483   1.1450909\n",
      "  2.5845077   0.8008674  -1.0638078   0.6531345   2.2798984  -0.50601417\n",
      "  3.431377    0.08835929 -1.0834138  -2.2189693   1.2473469   0.36237058\n",
      "  0.83757216 -0.08804901  1.6457077  -0.5591198   0.554735    0.9320045\n",
      " -0.49623123 -0.06286693  0.32738355 -0.8722086   1.217688    0.17894688\n",
      "  0.62178355  0.26221249  1.399246   -0.5461321  -0.21947832  0.8566192\n",
      "  1.0562919   0.96123827  0.6450779  -0.6428576   0.03833745  2.015609\n",
      " -0.44299278 -0.47501665 -0.3401148   0.9631085   0.87619156  0.8260153\n",
      "  0.11005365  0.01982115  0.7699329   0.11367665  1.841508   -0.16198556\n",
      "  0.7679334   1.2147143   0.5665775   0.9539859  -1.0892584  -0.2965779\n",
      " -2.006946   -0.38140416 -0.6474524   0.12591663  0.881165   -0.24852733\n",
      " -0.80441886 -0.3500497  -0.13197556 -0.22427313  0.02687847  0.41098413\n",
      "  0.6268808   0.5248238   0.4562626  -0.74687576 -0.509361    0.21529076\n",
      "  0.4430241   2.1292026   0.40022463 -0.372525    1.9037824   1.9481585\n",
      " -1.0969247  -0.6094064   2.903189   -2.4290512  -0.924845    0.7735348\n",
      "  1.0293605   0.10293958  0.74915415  1.249762   -0.4562513  -0.62231356\n",
      "  3.5284274  -0.9660428  -0.14782733  0.61365473  0.910629    0.94106036\n",
      "  0.7825065   1.2314517   2.4033675   1.814043   -0.50682604  0.12374875\n",
      " -0.75540966  1.4066936   1.6025727   0.10578886  0.8026868   0.2545827\n",
      "  0.09085003  2.0402145   2.3383496  -0.2572634   0.29465926  0.5619716\n",
      "  0.6830181  -1.3013923  -2.3189316  -0.1636821   1.1669711  -0.16104418\n",
      "  1.2191101  -0.12356216  2.9246209   1.8428417   0.703659    1.853188\n",
      " -0.18179955  1.9883182   0.30197474 -0.97020394 -1.223947   -0.7044847\n",
      " -0.22827418 -0.4647033   0.5062255  -0.06764983 -0.35144106  0.9368277\n",
      "  1.5072157  -0.90002596  1.69052     3.6433585  -0.38545966  1.1181878\n",
      "  1.1171176   0.11488142  2.0956404   0.9734304  -0.2427127  -0.09198844\n",
      " -0.72427845 -1.3356905  -0.6141448  -0.14964557 -0.7663279   0.4649003\n",
      "  0.6887711   0.21613912  1.6419059   0.57694095 -0.18719183  1.5286746\n",
      " -1.0184965  -0.03194941 -0.33639932  0.5405669  -0.5620906   0.9653842\n",
      " -1.9195178   5.417093    1.6773349  -2.4865294   2.6914103  -0.9805101\n",
      "  0.6830288   0.80520624 -1.3594232  -0.8273605   0.88702685 -0.64160466\n",
      "  0.73109114 -0.4566581   0.8308649   0.6403743  -0.37997523  0.37352127\n",
      " -0.06618999 -1.0298934   0.98890656  0.06561258  0.10430623 -0.87704825\n",
      "  0.12507582  1.278145    0.25769082  0.21814749  0.7497482  -0.81621623\n",
      "  0.21928886  0.06835042  0.10621262 -0.84037066  0.98449326  0.9470547\n",
      " -0.04138997 -1.4971877   0.28159732 -2.1181736 ]\n",
      "[0.5993145  0.7433219  0.47241008 1.1317798  0.7680058  1.0383985\n",
      " 0.67668736 0.5002794  0.5644358  0.3862997  0.46691588 0.9054957\n",
      " 1.0901282  0.7043091  1.8265321  0.59321    0.7000155  1.312284\n",
      " 0.89361674 0.6004458  0.9594824  0.5994677  0.962549   1.5134538\n",
      " 1.0463738  0.6249353  0.586553   0.5339186  1.0366323  0.52635884\n",
      " 0.7390012  1.5542264  0.8702426  1.4075818  1.3282155  0.94118166\n",
      " 0.7772181  0.76421213 0.75373775 1.2237054  0.9683121  0.683835\n",
      " 0.95801055 0.6575709  0.93231106 0.9614017  0.83202654 0.78011537\n",
      " 1.0127088  1.3423274  2.313905   0.68551433 0.90694726 0.4734481\n",
      " 1.1285026  0.9258243  0.5177042  0.91360694 0.4417614  1.5133116\n",
      " 0.6165678  0.8111482  1.0278535  1.4462641  1.1129448  0.5863972\n",
      " 1.1881067  0.7538637  1.1710752  1.5616789  0.91238934 0.59244144\n",
      " 0.39954868 0.9950548  0.5630527  0.6534363  0.7547655  0.7915434\n",
      " 0.66711277 0.95169765 0.8690041  1.0933012  0.8890955  1.5928619\n",
      " 1.6801673  0.43863133 1.3106625  0.86294854 0.68430376 0.98237324\n",
      " 0.8530589  1.0882401  0.8782431  0.60117376 0.7031574  1.3387691\n",
      " 0.9875203  0.9247471  0.9608182  0.44652855 0.7399012  1.088121\n",
      " 0.8629007  0.37732744 0.7344807  1.0055567  1.2116417  0.6082219\n",
      " 0.60105413 0.7775753  0.7378408  0.4099958  0.7727032  0.83865196\n",
      " 0.54021066 0.6225531  0.9309145  0.91364616 0.60029864 0.8498156\n",
      " 0.6620454  0.64301157 0.6350774  0.7184488  0.805969   0.627371\n",
      " 0.7942486  0.6050198  1.4373517  1.16536    0.9110347  0.7296067\n",
      " 0.70463675 1.1228373  0.3854742  0.92900884 0.55212545 0.57967806\n",
      " 0.93001884 0.44783816 0.46973124 0.86975235 1.0592473  0.8265881\n",
      " 0.99123347 0.9902034  0.63016367 1.4827955  0.87487495 1.2296944\n",
      " 0.67105454 1.2454613  0.5342416  0.5004108  0.8923624  0.61014396\n",
      " 0.82663435 0.9758785  0.9627903  0.91100985 0.7832407  0.6794357\n",
      " 0.8449791  1.8006916  0.83395255 1.5315453  1.3125612  0.726595\n",
      " 0.8165167  0.5156394  0.92383915 0.6297396  0.98070085 0.48638716\n",
      " 0.38977697 1.3951528  0.7039699  0.5280704  0.76049125 0.98144794\n",
      " 1.0387621  0.9055165  0.724304   1.0224342  0.8022747  0.4116537\n",
      " 0.927275   0.52293104 1.5346736  0.90550816 0.8697379  1.2083136\n",
      " 0.7760347  0.6371469  0.7849488  1.4275066  0.9497316  0.59915936\n",
      " 0.7300581  2.2711008  0.66258496 0.85301805 0.9279181  0.985512\n",
      " 0.5242324  0.7376548  0.9649956  0.6084129  0.26891673 0.7847611\n",
      " 1.1319046  0.53560555 1.2927474  0.7241665  0.8858426  0.8944057\n",
      " 0.6639895  1.2638701  0.94712526 2.0052438  1.0111383  1.0546916\n",
      " 0.8930675  1.112659   1.01173    0.81250864 0.41926342 0.62028676\n",
      " 1.203818   0.7178779  0.5843421  0.8641997  1.0457503  0.81940305\n",
      " 0.61219716 0.6961267  0.81250507 0.76550525 0.99723864 1.0315828\n",
      " 0.79305834 0.8921242  0.8080349  1.2905972  0.700128   0.5518281\n",
      " 1.101418   1.0112405  0.80219775 1.2229544  0.68141174 0.5099024\n",
      " 0.9881549  1.0453433  0.77811015 0.6133744 ]\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "layer35 = model.layers[35]\n",
    "W = np.array(layer35.get_weights())\n",
    "print(W.shape)\n",
    "\n",
    "batchNorm = model.layers[35].get_weights()\n",
    "\n",
    "fMean = open(\"data/EleventhLayer/Eleventh_Layer_Mean.txt\", \"w\")\n",
    "fSD = open(\"data/EleventhLayer/Eleventh_Layer_StanDev.txt\", \"w\")\n",
    "fGamma = open(\"data/EleventhLayer/Eleventh_Layer_Gamma.txt\", \"w\")\n",
    "fBeta = open(\"data/EleventhLayer/Eleventh_Layer_Beta.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(len(batchNorm[0])):\n",
    "    fGamma.write(str(float(batchNorm[0][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[1])):\n",
    "    fBeta.write(str(float(batchNorm[1][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[2])):\n",
    "    fMean.write(str(float(batchNorm[2][i])) + \"\\n\")\n",
    "\n",
    "for i in range(len(batchNorm[3])):\n",
    "    # The Value for Epsilon is assumed to be 0.001 in this implementation\n",
    "    fSD.write(str(m.sqrt(float(batchNorm[3][i]) + 0.001)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(batchNorm[0])\n",
    "print(batchNorm[1])\n",
    "print(batchNorm[2])\n",
    "print(batchNorm[3])\n",
    "\n",
    "fMean.close()\n",
    "fSD.close()\n",
    "fGamma.close()\n",
    "fBeta.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkFile_EleventhLayer():\n",
    "    fExp = open('data/EleventhLayer/Eleventh_Layer_Output.txt', 'r')\n",
    "    fAct = open('data/EleventhLayer/output.txt', 'r')\n",
    "    \n",
    "    cExp = fExp.readlines()\n",
    "    cAct = fAct.readlines()\n",
    "    counter = 0\n",
    "    for i in range(len(cExp)):\n",
    "         if int(round(float(cAct[i].strip()))) != int(round(float(cExp[i].strip()))):\n",
    "            counter += 1\n",
    "            if(counter < 30):\n",
    "                print(str(cAct[i].strip()) + \"--->\" + str(cExp[i].strip()))\n",
    "            \n",
    "        \n",
    "    print(\"Number of mismatch - \" + str(counter))\n",
    "    \n",
    "    fExp.close()\n",
    "    fAct.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499997--->0.5000169\n",
      "2.500018--->2.4998946\n",
      "1.500016--->1.4999323\n",
      "1.499999--->1.5000088\n",
      "1.499973--->1.5000187\n",
      "1.500013--->1.4999523\n",
      "1.500216--->1.4995114\n",
      "0.499910--->0.5000042\n",
      "1.500382--->1.4999423\n",
      "1.500271--->1.499943\n",
      "0.500022--->0.49990854\n",
      "1.500150--->1.4998598\n",
      "0.500255--->0.49986613\n",
      "Number of mismatch - 13\n"
     ]
    }
   ],
   "source": [
    "checkFile_EleventhLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
